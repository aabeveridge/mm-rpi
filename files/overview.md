#Big Data, Tiny Computers: Making Data-Driven Methods Accessible with a Raspberry Pi

By [Aaron Beveridge](http://aaronbeveridge.com/) and [Nicholas M. Van Horn](http://nicholasvanhorn.com/)

![](./images/rpi.jpg)

The phrase “Big Data” often implies the use of research practices requiring massive supercomputers to process datasets too large for traditional methods of analysis. Advertisements from companies like IBM and Microsoft reinforce Big Data's intimidation factor, as these companies no doubt prefer researchers purchase their tools and analytics suites XXXname their actual analytics suitesXXX. The work involved in making data-driven methods more accessible extends far beyond software choices---the work extends all the way down to the computer hardware and to issues with the data itself. In academic research, we frequently ask questions that already-available datasets and ready-made software tools cannot answer, because producing data-driven scholarship rarely involves plugging a ready-made dataset into analytics software that produces a publishable result. Instead, data-driven methods often require an inventive *maker* approach when researchers investigate novel questions with previously unexplored data.

Like many other fields, digital rhetoricians have been exploring the value of Big Data (data science) methods for our own research practices. While this is due, in part, to the broad influence of the digital humanities (Ridolfo/Hartdavidson, Jockers), it also stems from the long-standing interdisciplinary nature of research questions that continue to emerge in digital rhetoric (Eyman). In addition to these widely varied questions, rhetoric and writing studies have a long-standing history of *making* to open new possibilities for the work we pursue (writing teachers writing software, other?). More recently, projects like the [Writing Studies Tree](https://www.writingstudiestree.org/), [Faciloscope](http://faciloscope.cal.msu.edu/facilitation/), [Hedge-O-Matic](http://hedgeomatic.cal.msu.edu/hedgeomatic/), and [MassMine](http://www.massmine.org/) exemplify the vast potential for the type of work that emerges from the intersection of data-driven methods and a maker research orientation.

This chapter extends this maker orientation into the realm of physical computing, and demonstrates how an inexpensive [Raspberry Pi](https://www.raspberrypi.org/) computer (~$35) can addresses key hardware and workflow issues for long-term data collection projects. As the creators of MassMine, the authors of this chapter have been developing tools that reduce the learning curve for scholars who need to collect or create novel datasets from digital sources. To date, our work has focused primarily on the software and programming issues associated with digital data collection. From a hardware perspective, however, long-term data collection activities---those which are needed to create large research-quality datasets---require researchers to have a computer or cloud server running for days, weeks, months, and potentially, years. As computing technology continues to get smaller and more affordable, tiny computers and microprocessors---like the Arduino and Raspberry Pi---open many possibilities for maker projects that utilize the modularity, portability, and efficiency of these devices.

*Efficiency*, in particular, can have many meanings for Big Data: computational efficiency (how many steps does it take for a given program to complete its task?), processing efficiency (how many processors are necessary or available for completing the steps of a program?), temporal efficiency (how long will it take for a program to complete its task?), power efficiency (how much electricity is needed to power the computer processors?), and work efficiency (what are the pragmatic restrictions for completing everything involved in a project?). While these are among the more important efficiency considerations for data analytics work, the problems they pose may result in competing and contradictory answers. For example, programs that use multiple processors, simultaneously, often produce the most efficient results when processing a large dataset, but these types of programs can be very difficult to design. A programmer could take 2 weeks to design and implement an advanced program that uses multiple processors to run 200% faster than a program that already exists. However, if the program that already exists could finish its task in 1 week's time, then the 2 weeks required to develop the new advanced program would result in 1 week of wasted time. Still, other considerations change how we might judge efficiency for this example. If the newly designed program will be used on a regular basis in the future---saving an accumulated amount of time as the new program is reused repeatedly---then the future time savings justifies the additional effort needed to design the new program. XXXIn other words, there are many competing issues involved in trying to efficiently collect and process data for research.XXX

For many researchers, the issues that effect the efficiency or sustainability of a research project are often more pragmatic in nature. For example, when collecting Twitter data for our *Digital Humanities Quarterly* article, "Attention Ecology: Trend Circulation and the Virality Threshold" (co-authored with Sean Morey), we tracked 17,343 unique trends over seventy-four days---accessing Twitter's Application Programming Interface (API) every 5 minutes (the maximum allowed by the API) to collect and archive data. The software used to access Twitter and manage the data collection was an early version of MassMine. While MassMine ran without any problems throughout the entire project, our team ran into multiple pragmatic issues with our hardware setup. When we first started to collect data, MassMine was running in the background on one of our personal laptops. Because MassMine uses limited system resources, MassMine was able to collect data without negatively effecting the performance of other software on the laptop. However, three glaring issues were revealed a couple days into the initial data collection: (1) the laptop could not be reset or turned off without having to restart the data collection, (2) MassMine needs to access the internet every 5 minutes to collect the maximum data allowed by Twitter---requiring a constant internet connection, and (3) other people were no longer able to borrow or use the computer to ensure that the data collection was not accidentally shut down. While these issues may seem trivial, over a long enough period of time even the most trivial of issues can hinder the feasibility of a research project. Given our intention to track Twitter trends for multiple months, we decided that a dedicated computer was a more feasible solution.

While there are other approaches that we could have used to collect data with MassMine, like using a university research computer cluster (supercomputer) or a running a cloud server on a system like Amazon's AWS, these options are only available to the most advanced researchers who have been trained to use these systems (and they often require grant funds or other financial support). Therefore, we developed a research workflow that uses a Raspberry Pi as a dedicated data collection server. XXXThis article...the sections that follow...XXX  


<a href="comparison.html"><button type="button">Next Page</button></a>
