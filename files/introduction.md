#Big Data, Tiny Computers: Making Data-Driven Methods Accessible with a Raspberry Pi

By [Aaron Beveridge](http://aaronbeveridge.com/) and [Nicholas M. Van Horn](http://nicholasvanhorn.com/)

![](./images/rpi.jpg)

The phrase “Big Data” often suggests research practices that require massive supercomputers that process datasets too large for traditional methods of analysis. Advertisements from companies like IBM and Microsoft reinforce Big Data's intimidation factor, as these companies no doubt prefer that researchers purchase their tools and analytics suites. Yet, the problems associated with making data-driven methods more accessible extend far beyond software and programming choices---the problems extend all the way down to hardware choices and issues with the data itself. In academic research, we frequently ask questions that already-available datasets and ready-made software tools cannot answer. Therefore, broad applications of data-driven methods for academic research require an inventive *maker* approach when investigating novel questions through previously unexplored data.

Like many other fields, digital rhetoricians have begun investigating the value of data science methods for our own research practices. While this is due, in part, to the broad influence of the digital humanities (Ridolfo/Hartdavidson, Jockers), it also stems from the long-standing interdisciplinary nature of the research questions that continue to emerge in our field (Eyman). However, in addition to the widely varied questions we investigate, there has also been a long history of making and inventing in order to further open new possibilities for the work we pursue (writing teachers writing software, other?). More recently, projects like the [Writing Studies Tree](https://www.writingstudiestree.org/), [Faciloscope](http://faciloscope.cal.msu.edu/facilitation/), [Hedge-O-Matic](http://hedgeomatic.cal.msu.edu/hedgeomatic/), and [MassMine](http://www.massmine.org/) exemplify the potential nexus of data-driven methods and a maker research orientation.

This chapter extends the maker orientation into the realm of physical computing, and demonstrates how an inexpensive [Raspberry Pi](https://www.raspberrypi.org/) computer (~$35) can addresses key hardware and workflow issues for long-term data collection projects. As the creators of MassMine, the authors of this chapter have been developing tools that reduce the learning curve for scholars who need to collect and create novel datasets from digital sources, but our work to date has focused primarily on the software and programming issues associated with digital data collection. From a hardware perspective, however, long-term data mining and web scraping activities---those which are needed to create large research-quality datasets---require researchers to have a computer or cloud server running for multiple days, weeks, months, and potentially, years. As computing technology continues to get smaller and more affordable, tiny computers and microprocessors---like the Arduino and the Raspberry Pi---open many possibilities for maker projects that utilize the modularity, portability, and efficiency of these devices.

*Efficiency*, in particular, can have many meanings for Big Data: computational efficiency (how many steps does it take for a given program to complete its task?), processing efficiency (how many processors are necessary or available for completing the steps of a program?), temporal efficiency (how long will it take for a program to complete its task?), power efficiency (how much electricity is needed to power the computer processors?), and work efficiency (what are the pragmatic restrictions for completing everything involved in a project?). While these are among the more important efficiency considerations for data analytics work, the problems they pose may result in competing and contradictory answers. For example, programs that use multiple processors, simultaneously, often produce the most efficient results when processing a large dataset, but these types of programs can be very difficult to design. A programmer could hypothetically take 2 weeks to design and implement an advanced program that runs 200% faster than a simple program that already exists. However, if the simple program could finish its task in 1 week's time, then the 2 weeks required to develop the new advanced program would result in 1 additional week of wasted time. Still, other temporal considerations change how we judge efficiency for this example. If the newly designed advanced program will be used on a regular basis in the future---saving an accumulated amount of time as the new program is reused repeatedly---then the future time savings justify the additional effort needed to design the new program.

While open source and code sharing initiatives (<---hyperlink/cite) go a long way toward ensuring that data analysts benefit from one another's programming developments---especially where processing and computational efficiencies are concerned---other efficiency issues act as barriers for scholars who utilize data-driven methods. The time and effort required to access or collect the data needed for a particular project can potentially stop a project before it ever begins. If the data needed to answer a specific research question cannot be accessed or collected in a feasible amount of time, then all other considerations are secondary. Without data, data science simply becomes an abstract exercise in statistics and programming.

In the next two sections of this chapter we show how to use a Raspberry Pi for collecting Google Trends and Wikipedia data for research. The goal is to present the "maker" approach to...

<a href="comparison.html"><button type="button">Next Page</button></a>
